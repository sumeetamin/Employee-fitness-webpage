{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "100ecb6f",
   "metadata": {},
   "source": [
    "# Stacking with Xgboost, Catboost and LGB with Random forest as meta model without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4cb4ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumeet\\anaconda3\\anaconda\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\Sumeet\\anaconda3\\anaconda\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n",
      "C:\\Users\\Sumeet\\anaconda3\\anaconda\\lib\\site-packages\\dask\\dataframe\\__init__.py:31: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 349\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.707878\n",
      "[LightGBM] [Info] Start training from score -1.618857\n",
      "[LightGBM] [Info] Start training from score -1.551759\n",
      "[LightGBM] [Info] Start training from score -1.593934\n",
      "[LightGBM] [Info] Start training from score -1.581701\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 349\n",
      "[LightGBM] [Info] Number of data points in the train set: 1280, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.707878\n",
      "[LightGBM] [Info] Start training from score -1.621226\n",
      "[LightGBM] [Info] Start training from score -1.552497\n",
      "[LightGBM] [Info] Start training from score -1.593934\n",
      "[LightGBM] [Info] Start training from score -1.578666\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000845 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 349\n",
      "[LightGBM] [Info] Number of data points in the train set: 1280, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.707878\n",
      "[LightGBM] [Info] Start training from score -1.617281\n",
      "[LightGBM] [Info] Start training from score -1.552497\n",
      "[LightGBM] [Info] Start training from score -1.593934\n",
      "[LightGBM] [Info] Start training from score -1.582461\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 349\n",
      "[LightGBM] [Info] Number of data points in the train set: 1280, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.707878\n",
      "[LightGBM] [Info] Start training from score -1.617281\n",
      "[LightGBM] [Info] Start training from score -1.552497\n",
      "[LightGBM] [Info] Start training from score -1.593934\n",
      "[LightGBM] [Info] Start training from score -1.582461\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 349\n",
      "[LightGBM] [Info] Number of data points in the train set: 1280, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.707878\n",
      "[LightGBM] [Info] Start training from score -1.617281\n",
      "[LightGBM] [Info] Start training from score -1.552497\n",
      "[LightGBM] [Info] Start training from score -1.593934\n",
      "[LightGBM] [Info] Start training from score -1.582461\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 349\n",
      "[LightGBM] [Info] Number of data points in the train set: 1280, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.707878\n",
      "[LightGBM] [Info] Start training from score -1.621226\n",
      "[LightGBM] [Info] Start training from score -1.548813\n",
      "[LightGBM] [Info] Start training from score -1.593934\n",
      "[LightGBM] [Info] Start training from score -1.582461\n",
      "Accuracy without PCA: 0.2375\n",
      "Precision without PCA: 0.23217407367996173\n",
      "Recall without PCA: 0.23665179189151792\n",
      "F1 Score without PCA: 0.23287751913192906\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('Final_GP_Dataset5.csv')\n",
    "\n",
    "# Replace special JSON characters in column names\n",
    "data.columns = [col.replace('{', '').replace('}', '').replace('[', '').replace(']', '').replace(':', '').replace(',', '') for col in data.columns]\n",
    "\n",
    "# Preprocess data\n",
    "label_encoders = {}\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop('Do you regularly feel physically or emotionally exhausted?', axis=1)\n",
    "y = data['Do you regularly feel physically or emotionally exhausted?']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Base models\n",
    "estimators = [\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')),\n",
    "    ('cat', CatBoostClassifier(verbose=0)),\n",
    "    ('lgb', LGBMClassifier())\n",
    "]\n",
    "\n",
    "# Stacking model without PCA\n",
    "stack_model = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(random_state=42))\n",
    "stack_model.fit(X_train, y_train)\n",
    "y_pred = stack_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy without PCA: {accuracy}')\n",
    "print(f'Precision without PCA: {precision}')\n",
    "print(f'Recall without PCA: {recall}')\n",
    "print(f'F1 Score without PCA: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc9f3b0",
   "metadata": {},
   "source": [
    "# Stacking with Xgboost, Catboost and LGB with Random forest as meta model with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411116dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 349\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.707878\n",
      "[LightGBM] [Info] Start training from score -1.618857\n",
      "[LightGBM] [Info] Start training from score -1.551759\n",
      "[LightGBM] [Info] Start training from score -1.593934\n",
      "[LightGBM] [Info] Start training from score -1.581701\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 349\n",
      "[LightGBM] [Info] Number of data points in the train set: 1280, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.707878\n",
      "[LightGBM] [Info] Start training from score -1.621226\n",
      "[LightGBM] [Info] Start training from score -1.552497\n",
      "[LightGBM] [Info] Start training from score -1.593934\n",
      "[LightGBM] [Info] Start training from score -1.578666\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 349\n",
      "[LightGBM] [Info] Number of data points in the train set: 1280, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.707878\n",
      "[LightGBM] [Info] Start training from score -1.617281\n",
      "[LightGBM] [Info] Start training from score -1.552497\n",
      "[LightGBM] [Info] Start training from score -1.593934\n",
      "[LightGBM] [Info] Start training from score -1.582461\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003110 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 349\n",
      "[LightGBM] [Info] Number of data points in the train set: 1280, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.707878\n",
      "[LightGBM] [Info] Start training from score -1.617281\n",
      "[LightGBM] [Info] Start training from score -1.552497\n",
      "[LightGBM] [Info] Start training from score -1.593934\n",
      "[LightGBM] [Info] Start training from score -1.582461\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000586 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 349\n",
      "[LightGBM] [Info] Number of data points in the train set: 1280, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.707878\n",
      "[LightGBM] [Info] Start training from score -1.617281\n",
      "[LightGBM] [Info] Start training from score -1.552497\n",
      "[LightGBM] [Info] Start training from score -1.593934\n",
      "[LightGBM] [Info] Start training from score -1.582461\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 349\n",
      "[LightGBM] [Info] Number of data points in the train set: 1280, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.707878\n",
      "[LightGBM] [Info] Start training from score -1.621226\n",
      "[LightGBM] [Info] Start training from score -1.548813\n",
      "[LightGBM] [Info] Start training from score -1.593934\n",
      "[LightGBM] [Info] Start training from score -1.582461\n",
      "Accuracy without PCA: 0.2375\n",
      "Precision without PCA: 0.23217407367996173\n",
      "Recall without PCA: 0.23665179189151792\n",
      "F1 Score without PCA: 0.23287751913192906\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('Final_GP_Dataset5.csv')\n",
    "\n",
    "# Replace special JSON characters in column names\n",
    "data.columns = [col.replace('{', '').replace('}', '').replace('[', '').replace(']', '').replace(':', '').replace(',', '') for col in data.columns]\n",
    "\n",
    "# Preprocess data\n",
    "label_encoders = {}\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop('Do you regularly feel physically or emotionally exhausted?', axis=1)\n",
    "y = data['Do you regularly feel physically or emotionally exhausted?']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Base models\n",
    "estimators = [\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')),\n",
    "    ('cat', CatBoostClassifier(verbose=0)),\n",
    "    ('lgb', LGBMClassifier())\n",
    "]\n",
    "\n",
    "# Stacking model without PCA\n",
    "stack_model = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(random_state=42))\n",
    "stack_model.fit(X_train, y_train)\n",
    "y_pred = stack_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy without PCA: {accuracy}')\n",
    "print(f'Precision without PCA: {precision}')\n",
    "print(f'Recall without PCA: {recall}')\n",
    "print(f'F1 Score without PCA: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bb074e",
   "metadata": {},
   "source": [
    " # Stacking with Xgboost, Catboost and LGB with Bagging with Support Vector machine as meta model without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df70fb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 349\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -1.707878\n",
      "[LightGBM] [Info] Start training from score -1.618857\n",
      "[LightGBM] [Info] Start training from score -1.551759\n",
      "[LightGBM] [Info] Start training from score -1.593934\n",
      "[LightGBM] [Info] Start training from score -1.581701\n",
      "Accuracy: 0.2525\n",
      "Precision: 0.28438980737718467\n",
      "Recall: 0.2550221392002214\n",
      "F1 Score: 0.24462004809895013\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from mlxtend.classifier import StackingClassifier as MLXStackingClassifier\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('Final_GP_Dataset5.csv')\n",
    "data.columns = [col.replace('{', '').replace('}', '').replace('[', '').replace(']', '').replace(':', '').replace(',', '') for col in data.columns]\n",
    "\n",
    "# Label encoding for all categorical features\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    data[column] = LabelEncoder().fit_transform(data[column])\n",
    "\n",
    "# Adjusting label encoding for the target to start from 0\n",
    "target_col = 'Do you regularly feel physically or emotionally exhausted?'\n",
    "data[target_col] = LabelEncoder().fit_transform(data[target_col])\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop(target_col, axis=1)\n",
    "y = data[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Base models\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "lgb = LGBMClassifier()\n",
    "\n",
    "# Meta-model\n",
    "svm = SVC()\n",
    "\n",
    "# Stacking model using MLXtend\n",
    "stack = MLXStackingClassifier(classifiers=[xgb, cat, lgb], meta_classifier=svm, use_probas=False, average_probas=False)\n",
    "stack.fit(X_train, y_train)\n",
    "y_pred = stack.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddbee73",
   "metadata": {},
   "source": [
    "#  Stacking with Xgboost, Catboost and LGB with Bagging with Support Vector machine as meta model with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8582cb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score -1.707878\n",
      "[LightGBM] [Info] Start training from score -1.618857\n",
      "[LightGBM] [Info] Start training from score -1.551759\n",
      "[LightGBM] [Info] Start training from score -1.593934\n",
      "[LightGBM] [Info] Start training from score -1.581701\n",
      "Accuracy with PCA: 0.225\n",
      "Precision with PCA: 0.22648771304168172\n",
      "Recall with PCA: 0.2318022692680227\n",
      "F1 Score with PCA: 0.22560674558015864\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from mlxtend.classifier import StackingClassifier as MLXStackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('Final_GP_Dataset5.csv')\n",
    "data.columns = [col.replace('{', '').replace('}', '').replace('[', '').replace(']', '').replace(':', '').replace(',', '') for col in data.columns]\n",
    "\n",
    "# Label encoding for all categorical features\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    data[column] = LabelEncoder().fit_transform(data[column])\n",
    "\n",
    "# Adjusting label encoding for the target to start from 0\n",
    "target_col = 'Do you regularly feel physically or emotionally exhausted?'\n",
    "data[target_col] = LabelEncoder().fit_transform(data[target_col])\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop(target_col, axis=1)\n",
    "y = data[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Base models\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "lgb = LGBMClassifier()\n",
    "\n",
    "# Meta-model\n",
    "svm = SVC()\n",
    "\n",
    "# Stacking model using MLXtend with PCA\n",
    "stack_pca = MLXStackingClassifier(classifiers=[xgb, cat, lgb], meta_classifier=svm, use_probas=False, average_probas=False)\n",
    "stack_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = stack_pca.predict(X_test_pca)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "precision_pca = precision_score(y_test, y_pred_pca, average='macro')\n",
    "recall_pca = recall_score(y_test, y_pred_pca, average='macro')\n",
    "f1_pca = f1_score(y_test, y_pred_pca, average='macro')\n",
    "\n",
    "print(f'Accuracy with PCA: {accuracy_pca}')\n",
    "print(f'Precision with PCA: {precision_pca}')\n",
    "print(f'Recall with PCA: {recall_pca}')\n",
    "print(f'F1 Score with PCA: {f1_pca}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49601f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aab0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
